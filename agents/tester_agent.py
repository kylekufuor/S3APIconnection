"""Tester Agent implementation for executing and validating generated scripts."""

import asyncio
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
from crewai import Crew, Task

from .base_agent import BaseCSVAgent


class TesterAgent(BaseCSVAgent):
    """
    The Tester Agent executes and validates generated Python scripts.

    This agent acts as a Quality Assurance Engineer, running the generated
    scripts with uv and comparing outputs against expected results.
    """

    def __init__(self) -> None:
        super().__init__(
            name="Tester",
            role="Quality Assurance Engineer",
            goal="To test the Python script generated by the Coder Agent and verify that it produces the expected output.",
            backstory="""You are a meticulous Quality Assurance Engineer with a passion for automated testing. 
            You are an expert in validating data and ensuring that software meets its requirements. You have 
            extensive experience with uv and Python script execution. You pay attention to edge cases and 
            provide detailed feedback on test results. Your testing approach is thorough and systematic.""",
        )

    async def execute_task(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the testing task.

        Args:
            task_data: Dictionary containing:
                - generated_script_path: The generated Python script path
                - input_file_path: Path to the input CSV file
                - expected_output_file_path: Path to the expected output CSV file
                - job_id: Unique identifier for the job

        Returns:
            Dictionary containing test results and validation
        """

        generated_script_path = task_data["generated_script_path"]
        input_file_path = task_data["input_file_path"]
        expected_output_file_path = Path(task_data["expected_output_file_path"])
        job_id = task_data["job_id"]

        self.log_execution_start(f"Testing generated script for job {job_id}")

        try:
            # Execute the script and capture output
            execution_result = await self._execute_script_with_uv(generated_script_path, input_file_path)

            if not execution_result["success"]:
                # Propose concrete fix suggestions using script content, input, and expected output
                fix_feedback, fix_text = await self._propose_fix_suggestions(
                    execution_result.get("error", ""),
                    generated_script_path,
                    input_file_path,
                    expected_output_file_path,
                )

                result = {
                    "success": False,
                    "test_passed": False,
                    "error": execution_result.get("error", "Script execution failed"),
                    "execution_output": execution_result.get("output", ""),
                    "comparison_result": None,
                    "feedback_for_coder": fix_feedback,
                    "test_report": fix_text,
                }
                self.log_execution_end(False, f"Script execution failed: {execution_result['error']}")
                return result

            # Compare output with expected results
            comparison_result = await self._compare_outputs(execution_result["csv_output"], expected_output_file_path)

            # Generate test report
            test_report = await self._generate_test_report(
                execution_result, comparison_result, input_file_path, expected_output_file_path
            )

            success = comparison_result["match"]
            result = {
                "success": True,
                "test_passed": success,
                "execution_output": execution_result["output"],
                "csv_output": execution_result["csv_output"],
                "comparison_result": comparison_result,
                "test_report": test_report,
                "execution_time": execution_result.get("execution_time", 0),
            }

            status_msg = "passed" if success else "failed"
            self.log_execution_end(True, f"Testing completed - {status_msg}")
            return result

        except Exception as e:
            error_msg = f"Failed to test generated script: {str(e)}"
            self.log_execution_end(False, error_msg)
            return {
                "success": False,
                "test_passed": False,
                "error": error_msg,
                "execution_output": "",
                "comparison_result": None,
            }

    async def _execute_script_with_uv(self, script_path: str, input_file_path: str) -> Dict[str, Any]:
        """Execute the script using uv and capture output."""
        try:
            import time

            start_time = time.time()

            # Execute the script with uv from project root
            from pathlib import Path

            from utils.file_handlers import safe_file_path, validate_file_exists

            project_root = Path(__file__).parent.parent.parent  # Go up to project root

            # Use the input file path provided by the workflow
            input_file = Path(input_file_path)
            script_file = Path(script_path)

            # Validate input file
            input_exists, input_error = validate_file_exists(input_file, "input")
            if not input_exists:
                return {
                    "success": False,
                    "error": input_error,
                    "output": "",
                    "stderr": "",
                }

            # Validate script file
            script_exists, script_error = validate_file_exists(script_file, "script")
            if not script_exists:
                return {
                    "success": False,
                    "error": script_error,
                    "output": "",
                    "stderr": "",
                }

            result = await asyncio.create_subprocess_exec(
                "uv",
                "run",
                safe_file_path(script_file),
                safe_file_path(input_file),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=project_root,
            )

            stdout, stderr = await result.communicate()
            execution_time = time.time() - start_time

            stdout_text = stdout.decode("utf-8")
            stderr_text = stderr.decode("utf-8")

            if result.returncode != 0:
                return {
                    "success": False,
                    "error": f"Script execution failed with return code {result.returncode}: {stderr_text}",
                    "output": stdout_text,
                    "stderr": stderr_text,
                    "execution_time": execution_time,
                }

            return {
                "success": True,
                "output": stdout_text,
                "stderr": stderr_text,
                "csv_output": stdout_text.strip(),
                "execution_time": execution_time,
            }

        except FileNotFoundError:
            return {
                "success": False,
                "error": "uv command not found. Please ensure uv is installed and in PATH.",
                "output": "",
                "stderr": "",
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Unexpected error during script execution: {str(e)}",
                "output": "",
                "stderr": "",
            }

    async def _compare_outputs(self, actual_csv: str, expected_output_path: Path) -> Dict[str, Any]:
        """Compare the actual CSV output with the expected output."""
        try:
            # Load expected output
            expected_df = pd.read_csv(expected_output_path)

            # Parse actual output
            from io import StringIO

            actual_df = pd.read_csv(StringIO(actual_csv))

            # Basic comparison
            shapes_match = actual_df.shape == expected_df.shape
            columns_match = list(actual_df.columns) == list(expected_df.columns)

            differences = []
            suggestions = []
            feedback_for_coder = []

            if not shapes_match:
                differences.append(f"Shape mismatch: actual {actual_df.shape} vs expected {expected_df.shape}")

            if not columns_match:
                differences.append(
                    f"Column mismatch: actual {list(actual_df.columns)} vs expected {list(expected_df.columns)}"
                )

            # Data comparison (if shapes and columns match)
            data_match = False
            if shapes_match and columns_match:
                try:
                    # Try exact comparison first
                    data_match = actual_df.equals(expected_df)

                    if not data_match:
                        # Check for near-equal values (for floating point comparisons)
                        data_match = actual_df.fillna("").astype(str).equals(expected_df.fillna("").astype(str))

                        if not data_match:
                            # Find specific differences
                            diff_mask = actual_df != expected_df
                            if diff_mask.any().any():
                                diff_locations = []
                                value_mapping_issues = []

                                for col in actual_df.columns:
                                    if diff_mask[col].any():
                                        diff_rows = diff_mask[col][diff_mask[col]].index.tolist()
                                        for row in diff_rows[:5]:  # Show first 5 differences
                                            actual_val = actual_df.loc[row, col]
                                            expected_val = expected_df.loc[row, col]
                                            diff_locations.append(
                                                f"Row {row}, Column '{col}': actual='{actual_val}' vs expected='{expected_val}'"
                                            )

                                            # Generate dynamic feedback for value mapping issues
                                            if isinstance(actual_val, str) and isinstance(expected_val, str):
                                                if actual_val != expected_val:
                                                    value_mapping_issues.append(
                                                        f"Column '{col}': Use exact value '{expected_val}' instead of '{actual_val}'"
                                                    )

                                                    # Generate specific feedback for the coder agent
                                                    feedback_for_coder.append(
                                                        {
                                                            "column": col,
                                                            "actual_value": actual_val,
                                                            "expected_value": expected_val,
                                                            "issue_type": "value_mismatch",
                                                            "suggestion": f"Ensure column '{col}' uses exact value '{expected_val}' from expected output",
                                                        }
                                                    )

                                differences.extend(diff_locations)

                                # Add suggestions for value mapping issues
                                if value_mapping_issues:
                                    suggestions.extend(value_mapping_issues)
                                    suggestions.append(
                                        "Value mapping issues detected. Ensure exact output matching by using the precise values from the expected output."
                                    )
                except Exception as e:
                    differences.append(f"Data comparison error: {str(e)}")

            overall_match = shapes_match and columns_match and data_match

            return {
                "match": overall_match,
                "shapes_match": shapes_match,
                "columns_match": columns_match,
                "data_match": data_match,
                "differences": differences,
                "suggestions": suggestions,
                "feedback_for_coder": feedback_for_coder,
                "actual_shape": actual_df.shape,
                "expected_shape": expected_df.shape,
                "actual_columns": list(actual_df.columns),
                "expected_columns": list(expected_df.columns),
            }

        except Exception as e:
            return {
                "match": False,
                "error": f"Failed to compare outputs: {str(e)}",
                "differences": [f"Comparison failed: {str(e)}"],
                "suggestions": ["Check if the generated script produces valid CSV output"],
                "feedback_for_coder": [],
            }

    async def _propose_fix_suggestions(
        self,
        error_text: str,
        script_path: str,
        input_file_path: str,
        expected_output_file_path: Path,
    ) -> tuple[List[Dict[str, Any]], str]:
        """
        Generate actionable fix suggestions based on the error, script code, input file, and expected schema.
        Returns (feedback_for_coder, report_text).
        """
        try:
            script_code = Path(script_path).read_text(encoding="utf-8", errors="replace")
        except Exception:
            script_code = ""

        # Load expected columns for header detection guidance
        expected_columns: List[str] = []
        try:
            import pandas as pd  # local import to avoid top-level issues

            expected_columns = pd.read_csv(expected_output_file_path).columns.tolist()
        except Exception:
            expected_columns = []

        # Quick input scan to suggest delimiter and header heuristics
        header_probe: str = ""
        delim_suggestion = ","
        try:
            with open(input_file_path, "r", encoding="utf-8", errors="replace") as f:
                first_lines = f.read().splitlines()[:200]
            # Count delimiters and propose
            candidates = [",", "|", "\t", ";"]
            counts = {d: sum(line.count(d) for line in first_lines) for d in candidates}
            delim_suggestion = max(counts, key=counts.get)
            # If we know expected columns, find a line containing most of them
            if expected_columns:
                best_idx = -1
                best_hits = -1
                for idx, line in enumerate(first_lines):
                    hits = sum(1 for col in expected_columns if col.lower() in line.lower())
                    if hits > best_hits:
                        best_hits = hits
                        best_idx = idx
                if best_idx >= 0:
                    header_probe = f"Potential header at line {best_idx + 1} with {best_hits}/{len(expected_columns)} expected columns present."
        except Exception:
            pass

        # Build feedback items for coder to modify pre-cleaner and loading
        feedback: List[Dict[str, Any]] = []

        # Generic execution error remediation
        feedback.append(
            {
                "issue_type": "execution_error",
                "suggestion": "Harden pre-cleaning: detect header by matching the majority of expected output column names in a single raw line; ignore all lines above it; normalize delimiter; keep records with exact field count.",
                "error_details": error_text,
            }
        )

        if expected_columns:
            feedback.append(
                {
                    "issue_type": "header_detection",
                    "suggestion": (
                        "Search input for a line that contains most of these columns and treat it as header: "
                        + ", ".join(expected_columns[:8])
                        + ("..." if len(expected_columns) > 8 else "")
                    ),
                }
            )

        # Delimiter and field count suggestion
        feedback.append(
            {
                "issue_type": "delimiter_detection",
                "suggestion": f"Auto-detect delimiter (likely '{delim_suggestion}') and enforce exact field count equal to len(expected columns). Use engine='python', on_bad_lines='skip'.",
            }
        )

        # Concrete code-level hints
        feedback.append(
            {
                "issue_type": "code_change",
                "suggestion": (
                    "In preclean_input_to_temp_csv: strip prose/comment lines, detect header via expected column match, "
                    "trim extra tokens to expected count, convert delimiter to comma, keep ALL rows until EOF, then read with pandas using robust args."
                ),
            }
        )

        report_lines = [
            "Tester diagnostic and proposed fix:",
            f"- Error: {error_text}",
            f"- Suggested delimiter: {delim_suggestion}",
            f"- {header_probe}" if header_probe else "- Header probe: not enough context",
            "- Key remediation: match expected columns in a single header line; ignore lines above; enforce exact field count; normalize delimiter; robust pandas read.",
        ]

        if script_code:
            report_lines.append("- Script snippet (first 20 lines) to adjust pre-cleaning:")
            report_lines.extend(script_code.splitlines()[:20])

        return feedback, "\n".join(report_lines)

    async def _generate_test_report(
        self,
        execution_result: Dict[str, Any],
        comparison_result: Dict[str, Any],
        input_file_path: str,
        expected_output_path: Path,
    ) -> str:
        """Generate a detailed test report with actionable feedback."""

        # Create the task for generating a comprehensive report
        report_task = Task(
            description=self._build_report_prompt(
                execution_result, comparison_result, input_file_path, expected_output_path
            ),
            agent=self.agent,
            expected_output="A detailed test report with analysis and recommendations",
        )

        # Create a crew to execute the task
        crew = Crew(agents=[self.agent], tasks=[report_task], verbose=False)

        # Execute the task through the crew
        result = crew.kickoff()
        return str(result)

    def _build_report_prompt(
        self,
        execution_result: Dict[str, Any],
        comparison_result: Dict[str, Any],
        input_file_path: str,
        expected_output_path: Path,
    ) -> str:
        """Build the prompt for the test report generation."""

        success_status = "PASSED" if comparison_result.get("match", False) else "FAILED"
        differences = comparison_result.get("differences", [])
        suggestions = comparison_result.get("suggestions", [])
        feedback_for_coder = comparison_result.get("feedback_for_coder", [])
        differences_text = "\n".join([f"- {diff}" for diff in differences]) if differences else "None"
        suggestions_text = "\n".join([f"- {suggestion}" for suggestion in suggestions]) if suggestions else "None"

        # Generate specific feedback for the coder agent
        coder_feedback = ""
        if feedback_for_coder:
            coder_feedback = "SPECIFIC FEEDBACK FOR CODER AGENT:\n"
            for feedback in feedback_for_coder:
                coder_feedback += f"- {feedback['suggestion']}\n"
            coder_feedback += "\n"

        prompt = f"""
        You are a Data Quality Assurance Expert. Analyze the test results and provide comprehensive feedback on the data transformation.

        TEST EXECUTION SUMMARY:
        - Status: {success_status}
        - Execution Time: {execution_result.get("execution_time", "Unknown")} seconds
        - Input File: {input_file_path}
        - Expected Output File: {expected_output_path.name}
        
        COMPARISON RESULTS:
        - Overall Match: {comparison_result.get("match", False)}
        - Shape Match: {comparison_result.get("shapes_match", False)}
        - Columns Match: {comparison_result.get("columns_match", False)}
        - Data Match: {comparison_result.get("data_match", False)}
        
        ACTUAL vs EXPECTED:
        - Actual Shape: {comparison_result.get("actual_shape", "Unknown")}
        - Expected Shape: {comparison_result.get("expected_shape", "Unknown")}
        - Actual Columns: {comparison_result.get("actual_columns", [])}
        - Expected Columns: {comparison_result.get("expected_columns", [])}
        
        DIFFERENCES FOUND:
        {differences_text}
        
        SUGGESTIONS FOR FIXES:
        {suggestions_text}
        
        {coder_feedback}
        
        EXECUTION OUTPUT:
        {execution_result.get("output", "No output captured")}
        
        ANALYSIS FRAMEWORK:
        1. **Data Structure Validation**: Check if the output structure matches expectations
        2. **Value Accuracy**: Verify that transformed values are correct and properly formatted
        3. **Data Integrity**: Ensure logical relationships are preserved
        4. **Format Compliance**: Confirm output format matches specifications
        5. **Edge Case Handling**: Assess handling of null values and special cases
        
        Please provide:
        1. A summary of the test results and overall assessment
        2. Analysis of any failures or discrepancies with specific examples
        3. Recommendations for fixing issues, focusing on data transformation principles
        4. Assessment of the transformation quality and reliability
        5. Suggestions for improvements in data processing logic
        6. Specific guidance for the coder agent on how to fix identified issues
        7. If value mapping issues exist, propose solutions like using enums, constants, or validation
        
        Format the report in a clear, professional manner suitable for both technical and non-technical stakeholders.
        Focus on data transformation principles rather than specific implementation details.
        """

        return prompt.strip()
